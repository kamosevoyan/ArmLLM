{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Debugging Large Language Models (LLMs) with Captum</H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use Captum's (captum.ai) LLM API to explore the reasoning capabilities of LLaMa2 models.\n",
    "\n",
    "We will design prompts and test the model on continuation, classification, zero and few shot-learning tasks.\n",
    "\n",
    "The following tutorial is good reference point for the excersises described in this notebook: https://captum.ai/tutorials/Llama2_LLM_Attribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets install all dependancies that will help us to execute our code.\n",
    "\n",
    "In your python enviorment (e.g. conda) install pytorch-cuda (e.g.: `conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia`), captum, bytesandbits, transformers, huggingface accelerate.\n",
    "\n",
    "Request access to LLaMa2 (https://llama.meta.com/llama-downloads/) if you haven't already done so.\n",
    "\n",
    "You can use Hugging Face API to access model weights. If you choose that path, make sure that you're logged in into hugging face with `huggingface-cli`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise consists of three parts.\n",
    "\n",
    "1. In the first part of this exercise we aim to understand how the model learns input and output associations for a continuation task.\n",
    "This notebook contains example prompts and showcases how to use Captum with LLMs to explain those associations. You will be asked to write your own prompts and play with different configuration of the explainability API and prompt parametrization.\n",
    "\n",
    "2. In the second part, we use LLM as a binary classifier. We will examine whether Chain or Thought classifier agrees with the attribution-based explanations and how robust those explanations are.\n",
    "\n",
    "3. In the third part, we will examine the importance of few shot examples for a simple reasoning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    ShapleyValues,\n",
    "    ShapleyValueSampling,\n",
    "    LayerIntegratedGradients, \n",
    "    LLMAttribution, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    "    TextTemplateInput,\n",
    "    ProductBaselines,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure that CUDA is enabled\n",
    "print('Is CUDA Enabled: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining helper function to load the configs and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name, bnb_config=create_bnb_config()):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = \"10000MB\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load `Llama-3-13-chat-hf` model and the tokenizer associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\" \n",
    "# loading LlaMa2 Chat 13B\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define attribution methods and corresponding LLM API wrappers. \n",
    "\n",
    "We will use attribution API wrapper classes in combination with different perturbation and gradient-based algorithms to compute sample-based attributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attribution methods\n",
    "fa = FeatureAblation(model)\n",
    "sv = ShapleyValues(model)\n",
    "\n",
    "lig = LayerIntegratedGradients(model, model.model.embed_tokens)\n",
    "\n",
    "# LLM API for feaature ablation\n",
    "llm_attr_fa = LLMAttribution(fa, tokenizer)\n",
    "\n",
    "# LLM API for shapely values\n",
    "llm_attr_sc = LLMAttribution(sv, tokenizer)\n",
    "\n",
    "# LLM API for gradient-based methods\n",
    "llm_attr_grad = LLMGradientAttribution(lig, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Input-Output associations</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prompt LLM with a simple text that LLM will continue up to 20 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galileo_prompt = \"Galileo was born in Pisa, Italy. He discovered\"\n",
    "\n",
    "input_tokenized = tokenizer(galileo_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print('Tokenized Input: ', tokenizer.convert_ids_to_tokens(input_tokenized['input_ids'].detach().cpu().numpy()[0]))# \n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_tokenized[\"input_ids\"], max_new_tokens=20, temperature=0.1, do_sample=True)[0]\n",
    "    output_tokenized = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print('LLM Response: ', output_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to pass the tokenizer, the prompt and other parameters to Captum we wrap those arguments in `TextTokenInput` class. We then call `attribute` on an attribution algorithm by passing our `TextTokenInput` object. The latter outputs token-level explanations for each input-output token pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTokenInput(\n",
    "    galileo_prompt, \n",
    "    tokenizer,\n",
    "    skip_tokens=[1],  # skip the special token for the start of the text <s>\n",
    ")\n",
    "\n",
    "attr_res = llm_attr_fa.attribute(inp, gen_args={\"max_new_tokens\": 20, \"do_sample\": True, \"temperature\": 0.1}) \n",
    "# we can also pass a concrete target as an output such as: llm_attr_pert.attribute(inp, target=\"four largest moons of Jupiter\", ... )\n",
    "\n",
    "attr_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_token_attr` is an example helper function to plot pair-wise attributions. Feel free to use your favorite visualisation tool for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input tokens', attr_res.input_tokens)\n",
    "print('Output tokens', attr_res.output_tokens)\n",
    "attr_res.plot_token_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's mask words together. `TextTemplateInput` helps us to parametrize the input and filter the attribution for specific input tokens.\n",
    "This can be especially useful if the number of input and output tokens is relatively large.\n",
    "\n",
    "In this example we look into the associations between specific input tokens `Galileo`, `Pisa, Italy` and each output sub-word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTemplateInput(\n",
    "    template=\"{} was born in {}. He discovered\", \n",
    "    values=[\"Galileo\", \"Pisa, Italy\"],\n",
    ")\n",
    "attr_res = llm_attr_fa.attribute(inp, gen_args={\"max_new_tokens\": 20, \"do_sample\": True, \"temperature\": 0.01}) \n",
    "attr_res.plot_token_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same example as before but here we use specific baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = TextTemplateInput(\n",
    "    template=\"{} was born in {}. He discovered\", \n",
    "    values=[\"Galileo\", \"Pisa, Italy\"],\n",
    "    baselines=[\"Unknown\", \"Unknown, Unknown\"],\n",
    ")\n",
    "attr_res = llm_attr_fa.attribute(inp, gen_args={\"max_new_tokens\": 20, \"do_sample\": True, \"temperature\": 0.01}) \n",
    "attr_res.plot_token_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of specifying a specific baseline value we can specify a distribution of baselines. The attribution algorithm will sample uniformly random from that distribution\n",
    "and average the results across multiple baselines for more rebust outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = ProductBaselines(\n",
    "    {\n",
    "        \"name\":[\"James\", \"Alice\", \"Bob\", \"Sam\", \"Mary\", \"Eve\", \"Rob\"],\n",
    "        \"location\": [\"New York, USA\", \"Muenich, Germany\", \"London, UK\", \"Amsterdam, Netherlands\", \n",
    "                     \"Koeln, Germany\", \"San Francisco, USA\", \"LA, USA\"], \n",
    "    }\n",
    ")\n",
    "\n",
    "inp = TextTemplateInput(\n",
    "    template=\"{name} was born in {location}. He discovered\", \n",
    "    values={\"name\": \"Galileo\", \"location\": \"Pisa, Italy\"},\n",
    "    baselines=baselines,\n",
    ")\n",
    "attr_res = llm_attr_fa.attribute(inp, gen_args={\"max_new_tokens\": 20, \"do_sample\": True, \"temperature\": 0.01}) \n",
    "attr_res.plot_token_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Excercise 1</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define a continuation prompt of your choice.\n",
    "2. Compute and compare attributions for perturbation and gradient-based methods.\n",
    "3. Specify a target sequence that we want to attribute to, if the output is very long.\n",
    "4. Use `TextTemplateInput` to group input subwords together.\n",
    "5. Use masks to combine tokens in different parts of the input together.\n",
    "6. Observe the impact of the baseline selection on the attribution results. Leverage `ProductBaselines` to sample from a distribution of baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Classification<H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this excercise we will use LLMs as a binary sentiment classifier. We will examine whether chain of thought agrees on key sentiment tokens with the attribution-based explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example prompt which asks LLM to classify a movie review as `positive` or `negative` sentiment. In this examples chain of thought suggests that `aweful` is a key token associated with negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_rating_prompt = \"Your goal is to classify movie rating reviews as `positive` or `negative` sentiment. Is given text positive or negative sentiment ? \" \\\n",
    "                      \"'An awful film! It must have been up against some real stinkers to be nominated for the Golden Globe.'\"  \\\n",
    "                      \"Answer simply: 'positive' or 'negative'. Think step by step\"\n",
    "\n",
    "input_tokenized = tokenizer(movie_rating_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print('Tokenized input: ', tokenizer.convert_ids_to_tokens(input_tokenized['input_ids'].detach().cpu().numpy()[0]))# \n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_tokenized[\"input_ids\"], max_new_tokens=42, temperature=0.1, do_sample=True)[0]\n",
    "    output_tokenized = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print('LLM Response: ', output_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svs = ShapleyValueSampling(model)\n",
    "llm_attr_svs = LLMAttribution(svs, tokenizer)\n",
    "\n",
    "inp = TextTemplateInput(\n",
    "    template=\"Your goal is to classify movie rating reviews as `positive` or `negative` sentiment. Is given text positive or negative sentiment: \" \\\n",
    "                      \"'{} {} {} {} {} {} {} {} {} {} {} {} {} {} {} {} {} {} {}.\"  \\\n",
    "                      \"Answer simply: 'positive' or 'negative'\", \n",
    "    values=[\"An\", \"awful\", \"film!\", \"It\", \"must\", \"have\", \"been\", \"up\", \"against\", \"some\", \"real\", \"stinkers\", \"to\", \"be\", \"nominated\", \"for\", \"the\", \"Golden\", \"Globe\"],\n",
    ")\n",
    "attr_res = llm_attr_fa.attribute(inp, gen_args={\"max_new_tokens\": 20, \"do_sample\": True, \"temperature\": 0.01}) #, n_samples = 30) #\n",
    "\n",
    "attr_res.plot_token_attr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Excercise 2</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write instruction prompts that ask LLM to classify a specific text into binary or multiple classes. Use Chain of Thought to identify most influential tokens in the input. Compare the explanations provided for chain of thought reasoning with the attribution results.\n",
    "\n",
    "2. Experiment with different target values, baselines and input masks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Few shot examples - Reasoning Task<H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this excercise we will use Captum for a reasoning task with few shot examples. The task is to compute mod 2 on a sum of two numbers. We provide four examples and ask LLM to solve the task based on those examples. The goal is to estimate the importance of the few shot examples in the reasoning task.\n",
    "\n",
    "We then remove the most important example from the prompt and observe whether LLM is still able to solve the task correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt = \"Here are some examples of different operations:\" \\\n",
    "                        \"(1 + 1) mod 2 = 0 , \" \\\n",
    "                        \"(2 + 1) mod 2 = 1, \" \\\n",
    "                        \"(1 + 0) mod 2 = 1, \" \\\n",
    "                        \"(0 + 0) mod 2 = 0 \" \\\n",
    "                        \" What is (2 + 0) mod 2 = ?. Think step by step.\"\n",
    "\n",
    "input_tokenized = tokenizer(reasoning_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print('Tokenized input: ', tokenizer.convert_ids_to_tokens(input_tokenized['input_ids'].detach().cpu().numpy()[0]))# \n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_tokenized[\"input_ids\"], max_new_tokens=50, temperature=0.1, do_sample=True)[0]\n",
    "    output_tokenized = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print('LLM Response: ', output_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_fn(*examples):\n",
    "    main_prompt = \"What is (2 + 0) mod 2 = ?\"\n",
    "    subset = [elem for elem in examples if elem]\n",
    "    if not subset:\n",
    "        prompt = main_prompt\n",
    "    else:\n",
    "        prefix = \"Here are some examples of different arithmetic operations:\\n\"\n",
    "        prompt = prefix + \" \\n\".join(subset) + \"\\n \" + main_prompt\n",
    "    return \"[INST] \" + prompt + \"[/INST]\"\n",
    "\n",
    "input_examples = [\n",
    "        \"(1 + 1) mod 2 = 0\",\n",
    "        \"(2 + 1) mod 2 = 1, \",\n",
    "        \"(1 + 0) mod 2 = 1 \",\n",
    "        \"(0 + 0) mod 2 = 0 \"\n",
    "]\n",
    "inp = TextTemplateInput(\n",
    "    prompt_fn, \n",
    "    values=input_examples,\n",
    ")\n",
    "\n",
    "sv = ShapleyValues(model) \n",
    "\n",
    "sv_llm_attr = LLMAttribution(sv, tokenizer)\n",
    "\n",
    "attr_res = sv_llm_attr.attribute(inp, target=\"(2 + 0) mod 2 = 0\")\n",
    "\n",
    "attr_res.plot_token_attr(show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ` \"(2 + 1) mod 2 = 1, \"` is the most important out of all few-shot examples. Now, if we remove that example, the model no longer provides the correct answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt = \"Here are some examples of different operations:\" \\\n",
    "                        \"(1 + 1) mod 2 = 0 , \" \\\n",
    "                        \"(1 + 0) mod 2 = 1, \" \\\n",
    "                        \"(0 + 0) mod 2 = 0 \" \\\n",
    "                        \" What is (2 + 0) mod 2 = ?. Think step by step.\"\n",
    "\n",
    "input_tokenized = tokenizer(reasoning_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print('Tokenized input: ', tokenizer.convert_ids_to_tokens(input_tokenized['input_ids'].detach().cpu().numpy()[0]))# \n",
    "\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_tokenized[\"input_ids\"], max_new_tokens=50, temperature=0.1, do_sample=True)[0]\n",
    "    output_tokenized = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    print('LLM Response: ', output_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Excercise 3</H2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
