{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Indexing Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirekara/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================WARNING: DEPRECATED!==============================\n",
      "WARNING! This version of bitsandbytes is deprecated. Please switch to `pip install bitsandbytes` and the new repo: https://github.com/TimDettmers/bitsandbytes\n",
      "==============================WARNING: DEPRECATED!==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-07 13:26:47.437247: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-07 13:26:47.470542: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-07 13:26:47.470570: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-07 13:26:47.471393: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-07 13:26:47.477044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-07 13:26:48.294223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import tensordot\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "from torch import Tensor\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DPRContextEncoder\n",
    "from typing import List, Dict\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make it a huggingface dataset out of pure convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "imdb_dataset = Dataset.load_from_disk(\"imdb_top_10k_embeddings_dataset\")\n",
    "imdb_embeddings = np.load(\"imdb_top_10k_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'Movie Name', 'Rating', 'Runtime', 'Genre', 'Metascore', 'Plot', 'Directors', 'Stars', 'Votes', 'Gross', 'Link', 'text', 'embeddings'],\n",
       "    num_rows: 9999\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02105394,  0.06378748, -0.04121039, ...,  0.00074133,\n",
       "         0.00416954,  0.00915009],\n",
       "       [-0.0132991 ,  0.03195634, -0.04103773, ..., -0.02429469,\n",
       "         0.02605752,  0.00711353],\n",
       "       [ 0.02886778, -0.04718696, -0.02782897, ..., -0.01550656,\n",
       "        -0.01053093, -0.02435608],\n",
       "       ...,\n",
       "       [ 0.03597383, -0.04812222, -0.04116966, ...,  0.05863594,\n",
       "        -0.02354652,  0.01549213],\n",
       "       [ 0.03175337,  0.0090703 , -0.04926109, ...,  0.01373126,\n",
       "        -0.03838968, -0.05070541],\n",
       "       [ 0.01239666,  0.00556153, -0.02697754, ..., -0.01962082,\n",
       "        -0.02398801,  0.01530278]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_embedder(nn.Module):\n",
    "    def __init__(self, feat_extractor_name: str = ''):\n",
    "        \"\"\"Transformer Embedding model\n",
    "\n",
    "        Args:\n",
    "            feat_extractor_name (str, optional): Name of the feature extracator from HF hub or torch Hub.\n",
    "        \"\"\"        \n",
    "        super(Transformer_embedder, self).__init__()\n",
    "        \n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.feat_extractor_name = feat_extractor_name\n",
    "\n",
    "        if 'dpr' in feat_extractor_name.lower():\n",
    "            feat_extractor = DPRContextEncoder.from_pretrained(feat_extractor_name)\n",
    "        else:\n",
    "            feat_extractor = AutoModel.from_pretrained(feat_extractor_name)\n",
    "            \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(feat_extractor_name)\n",
    "\n",
    "        \n",
    "        self.normalize = True\n",
    "        self.feat_extractor = feat_extractor\n",
    "        self.embeding_shape = self.get_extractor_output_shape() \n",
    "                            \n",
    "\n",
    "    def get_extractor_output_shape(self):\n",
    "        last_layer = list(self.feat_extractor.named_children())[-1]\n",
    "\n",
    "        if hasattr( list(last_layer[1].modules())[1] , 'out_features'):\n",
    "            shape = list(last_layer[1].modules())[1].out_features\n",
    "        else:\n",
    "            shape = self.feat_extractor.config.hidden_size\n",
    "\n",
    "        return shape\n",
    "    \n",
    "    def mean_pooling(self, model_output:Tensor, attention_mask:Tensor):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def pool(self, embedding:Tensor, attention_mask:Tensor, pool_type:str = 'mean'):\n",
    "        \n",
    "        if 'mean' in pool_type:\n",
    "            pooled = self.mean_pooling(embedding, attention_mask)\n",
    "        else:\n",
    "            pooled = embedding.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def __call__(self, input_ids:Tensor, attention_mask:Tensor, labels: Tensor = None, **kwargs):\n",
    "\n",
    "        # print('input_ids.shape: ', input_ids.shape)\n",
    "        embedding = self.feat_extractor(input_ids, attention_mask)\n",
    "\n",
    "        if 'dpr' in self.feat_extractor_name.lower():\n",
    "            pooled = embedding.pooler_output\n",
    "        else:\n",
    "            pooled = self.pool(embedding, attention_mask, pool_type='mean')\n",
    "        # print('embedding.shape: ', embedding.last_hidden_state.shape)\n",
    "        # last_hidden_states = embedding.last_hidden_state\n",
    "        # print('last_hidden_states.shape: ', last_hidden_states.shape)\n",
    "        # pooled = self.pool(last_hidden_states, attention_mask, pool_type='mean')\n",
    "        # print('pooled.shape: ', pooled.shape)\n",
    "\n",
    "        if self.normalize:\n",
    "            pooled = F.normalize(pooled, p=2, dim=1)\n",
    "\n",
    "        # print(pooled.shape)\n",
    "        return pooled\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "\n",
    "#The following is a bigger model and might require slight modification in the code\n",
    "# follow this link for more details: https://huggingface.co/intfloat/e5-mistral-7b-instruct\n",
    "# model_ckpt = \"intfloat/e5-mistral-7b-instruct\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedder = Transformer_embedder(model_ckpt)\n",
    "embedder = embedder.to(device)\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = embedder.tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    embedder.eval()\n",
    "    with torch.inference_mode():\n",
    "        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "        model_output = embedder(**encoded_input)\n",
    "    return model_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Batman\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()[0]\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted File Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted file indexing is a technique used to index documents in a corpus. It is a data structure that maps terms to the documents that contain them. The inverted file index is a dictionary where the keys are terms and the values are lists of document IDs. The inverted file index is used to quickly find documents that contain a given term.\n",
    "\n",
    "Mathematically, the inverted file index is defined as follows:\n",
    "\n",
    "$$\n",
    "I = \\{t_1: [d_1, d_2, \\ldots, d_n], t_2: [d_1, d_2, \\ldots, d_n], \\ldots, t_m: [d_1, d_2, \\ldots, d_n]\\}\n",
    "$$\n",
    "\n",
    "Where $I$ is the inverted file index, $t_i$ is the $i$-th term, and $d_i$ is the $i$-th document ID.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\n",
      "Question:  Batman\n",
      "Top 5 similar movies:\n",
      "====================================================================================================\n",
      "Pulp Fiction\n",
      "The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of diner bandits intertwine in four tales of violence and redemption.\n",
      "====================================================================================================\n",
      "The Chaos Class\n",
      "Lazy, uneducated students share a very close bond. They live together in the dormitory, where they plan their latest pranks. When a new headmaster arrives, the students naturally try to overthrow him. A comic war of nitwits follows.\n",
      "====================================================================================================\n",
      "The Marathon Family\n",
      "The Topalovic family has been in the burial business for generations. When the old (150 yrs old) Pantelija dies, five generations of his heirs start to fight for the inheritance.\n",
      "====================================================================================================\n",
      "The Lord of the Rings: The Fellowship of the Ring\n",
      "A meek Hobbit from the Shire and eight companions set out on a journey to destroy the powerful One Ring and save Middle-earth from the Dark Lord Sauron.\n",
      "====================================================================================================\n",
      "Goodfellas\n",
      "The story of Henry Hill and his life in the mafia, covering his relationship with his wife Karen and his mob partners Jimmy Conway and Tommy DeVito.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "class IVFFlatIndexer:\n",
    "    def __init__(self, n_clusters: int = 100, n_init: int = 10, max_iter: int = 300):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_init = n_init\n",
    "        self.max_iter = max_iter\n",
    "        self.index = MiniBatchKMeans(\n",
    "            n_clusters=self.n_clusters,\n",
    "            n_init=self.n_init,\n",
    "            max_iter=self.max_iter,\n",
    "            init_size=3 * self.n_clusters,\n",
    "        )\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.index.fit(X)\n",
    "\n",
    "    def search(self, X, top_k=5):\n",
    "        return self.index.predict(X)\n",
    "    \n",
    "    def get_cluster_centers(self):\n",
    "        return self.index.cluster_centers_\n",
    "    \n",
    "    def get_cluster_labels(self):\n",
    "        return self.index.labels_\n",
    "    \n",
    "\n",
    "indexer = IVFFlatIndexer(n_clusters=100)\n",
    "indexer.fit(imdb_embeddings)\n",
    "\n",
    "print(indexer.search(question_embedding.reshape(1, -1)))\n",
    "\n",
    "def get_top_k_similar(question_embedding, embeddings, indexer, k=5):\n",
    "    cluster_id = indexer.search(question_embedding.reshape(1, -1))\n",
    "    cluster_embeddings = embeddings[indexer.get_cluster_labels() == cluster_id]\n",
    "    distances = np.dot(cluster_embeddings, question_embedding)\n",
    "    top_k_indices = np.argsort(distances)[::-1][:k]\n",
    "    return top_k_indices\n",
    "\n",
    "top_k_indices = get_top_k_similar(question_embedding, imdb_embeddings, indexer, k=5)\n",
    "top_k_indices\n",
    "\n",
    "print(\"Question: \", question)\n",
    "print(\"Top 5 similar movies:\")\n",
    "print(\"=\"*100)\n",
    "for idx in top_k_indices:\n",
    "    print(imdb_dataset[idx.item()][\"Movie Name\"])\n",
    "    print(imdb_dataset[idx.item()][\"Plot\"])\n",
    "    print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local sensitive hashing (LSH) is a technique used to find similar items in a large dataset. LSH is used to reduce the dimensionality of the data and to find similar items in the reduced space. LSH is used in many applications, such as near-duplicate detection, recommendation systems, and clustering.\n",
    "\n",
    "Mathematically, LSH is defined as follows:\n",
    "\n",
    "$$\n",
    "h(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{if } x \\geq t \\\\\n",
    "0 & \\text{if } x < t\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Where $h(x)$ is the hash function, $x$ is the input value, and $t$ is the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Batman\n",
      "Top 5 similar movies:\n",
      "====================================================================================================\n",
      "The Kids Are All Right\n",
      "Two children conceived by artificial insemination bring their biological father into their non-traditional family life.\n",
      "====================================================================================================\n",
      "Crimes and Misdemeanors\n",
      "An ophthalmologist's mistress threatens to reveal their affair to his wife while a married documentary filmmaker is infatuated with another woman.\n",
      "====================================================================================================\n",
      "The Battle of Britain\n",
      "In 1940, the British Royal Air Force fights a desperate battle to prevent the Luftwaffe from gaining air superiority over the English Channel as a prelude to a possible Axis invasion of the U.K.\n",
      "====================================================================================================\n",
      "Apocalypto\n",
      "As the Mayan kingdom faces its decline, a young man is taken on a perilous journey to a world ruled by fear and oppression.\n",
      "====================================================================================================\n",
      "Mr. Magorium's Wonder Emporium\n",
      "The young apprentice of a magical, eccentric toy store owner learns to believe in herself, and in her friends, upon learning some grave news about the future.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "class LSHIndexer:\n",
    "    def __init__(self, n_bits: int = 8):\n",
    "        self.n_bits = n_bits\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.index = np.packbits(X > 0, axis=1)\n",
    "\n",
    "    def search(self, X, top_k=5):\n",
    "        query = np.packbits(X > 0, axis=1)\n",
    "        distances = np.dot(query, self.index.T)\n",
    "        top_k_indices = np.argsort(distances)[::-1][:top_k]\n",
    "        return top_k_indices\n",
    "    \n",
    "    def get_index(self):\n",
    "        return self.index\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "indexer = LSHIndexer(n_bits=8)\n",
    "indexer.fit(imdb_embeddings)\n",
    "\n",
    "top_k_indices = indexer.search(question_embedding.reshape(1, -1))\n",
    "top_k_indices = top_k_indices[0][:5]\n",
    "\n",
    "\n",
    "print(\"Question: \", question)\n",
    "print(\"Top 5 similar movies:\")\n",
    "print(\"=\"*100)\n",
    "for idx in top_k_indices:\n",
    "    idx = idx.item()\n",
    "    print(imdb_dataset[idx][\"Movie Name\"])\n",
    "    print(imdb_dataset[idx][\"Plot\"])\n",
    "    print(\"=\"*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product quantization is a technique used to compress high-dimensional vectors into low-dimensional vectors. Product quantization is used to reduce the storage and computation costs of working with high-dimensional vectors. Product quantization is used in many applications, such as image retrieval, text search, and recommendation systems.\n",
    "\n",
    "Mathematically, product quantization is defined as follows:\n",
    "\n",
    "$$\n",
    "Q(x) = \\sum_{i=1}^{n} c_i \\cdot q_i(x)\n",
    "$$\n",
    "\n",
    "Where $Q(x)$ is the quantized vector, $x$ is the input vector, $c_i$ is the $i$-th codebook vector, and $q_i(x)$ is the $i$-th quantization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0]), array([0])]\n",
      "[0]\n",
      "Question:  Batman\n",
      "Top 5 similar movies:\n",
      "====================================================================================================\n",
      "The Shawshank Redemption\n",
      "Over the course of several years, two convicts form a friendship, seeking consolation and, eventually, redemption through basic compassion.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "class ProductQuantization:\n",
    "    def __init__(self, sentence_embeddings, n_subvectors=8):\n",
    "        \"\"\"\n",
    "        Product Quantization Index\n",
    "        \n",
    "        Args:\n",
    "            sentence_embeddings (np.array): Array of sentence embeddings\n",
    "            n_subvectors (int): Number of subvectors to divide each embedding into\n",
    "        \"\"\"\n",
    "        # Subspace dimensionality\n",
    "        self.d = sentence_embeddings.shape[1] // n_subvectors\n",
    "        # Number of subvectors\n",
    "        self.m = n_subvectors\n",
    "\n",
    "        # Generate random projection matrix\n",
    "        self.R = np.random.randn(self.d * self.m, self.d)\n",
    "        self.R /= np.linalg.norm(self.R, axis=0)\n",
    "\n",
    "        # Project sentence embeddings onto subspaces\n",
    "        self.subspace_embeddings = np.dot(sentence_embeddings, self.R)\n",
    "\n",
    "        # Quantize subspace embeddings\n",
    "        self.quantized_subspace_embeddings = np.round(self.subspace_embeddings / self.d)\n",
    "\n",
    "    def search(self, query_embedding, top_k=1):\n",
    "        \"\"\"\n",
    "        Search for the most similar sentences to a query embedding\n",
    "\n",
    "        Args:\n",
    "            query_embedding (np.array): Query embedding\n",
    "            top_k (int): Number of similar sentences to return\n",
    "\n",
    "        Returns:\n",
    "            list: List of indices of similar sentences\n",
    "        \"\"\" \n",
    "        # Project query embedding onto subspaces\n",
    "        query_subspace_embeddings = np.dot(query_embedding, self.R)\n",
    "\n",
    "        # Quantize query subspace embeddings\n",
    "        quantized_query_subspace_embeddings = np.round(query_subspace_embeddings / self.d)\n",
    "\n",
    "        # Find nearest neighbors in each subspace\n",
    "        nearest_neighbors = []\n",
    "        for i in range(self.m):\n",
    "            distances = np.linalg.norm(self.quantized_subspace_embeddings[:, i] - quantized_query_subspace_embeddings[i], ord=2)\n",
    "            nearest_neighbors.append(np.argsort(distances)[:top_k])\n",
    "\n",
    "        print(nearest_neighbors)\n",
    "        # Combine nearest neighbors from each subspace\n",
    "        combined_nearest_neighbors = set()\n",
    "        for neighbors in nearest_neighbors:\n",
    "            combined_nearest_neighbors.update(neighbors)\n",
    "\n",
    "        return list(combined_nearest_neighbors)\n",
    "    \n",
    "\n",
    "product_quantizer = ProductQuantization(imdb_embeddings, n_subvectors=8)\n",
    "top_k_indices = product_quantizer.search(question_embedding, top_k=5)\n",
    "print(top_k_indices)\n",
    "\n",
    "print(\"Question: \", question)\n",
    "print(\"Top 5 similar movies:\")\n",
    "print(\"=\"*100)\n",
    "for idx in top_k_indices:\n",
    "    idx = idx.item()\n",
    "    print(imdb_dataset[idx][\"Movie Name\"])\n",
    "    print(imdb_dataset[idx][\"Plot\"])\n",
    "    print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigable Small World Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigable small world graphs are a technique used to build a graph that can be used to efficiently search for similar items in a large dataset. Navigable small world graphs are used to reduce the search time and space required to find similar items in a large dataset. \n",
    "\n",
    "Mathematically, navigable small world graphs are defined as follows:\n",
    "\n",
    "$$\n",
    "G = (V, E)\n",
    "$$\n",
    "\n",
    "Where $G$ is the navigable small world graph, $V$ is the set of vertices, and $E$ is the set of edges.\n",
    "\n",
    "The algorithm for building a navigable small world graph is as follows:\n",
    "\n",
    "1. Initialize the graph with a single vertex.\n",
    "2. Add a new vertex to the graph.\n",
    "3. Connect the new vertex to the nearest vertex in the graph.\n",
    "4. Repeat steps 2 and 3 until all vertices are connected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirekara/anaconda3/lib/python3.11/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hnswlib\n",
      "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/kirekara/anaconda3/lib/python3.11/site-packages (from hnswlib) (1.23.5)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp311-cp311-linux_x86_64.whl size=202917 sha256=9210afb6014dba051395ee1d9874452c5a4bd60c3eac0cc5c7c61393b29b894c\n",
      "  Stored in directory: /home/kirekara/.cache/pip/wheels/ea/4e/27/39aebca9958719776e36fada290845a7ef10f053ad70e22ceb\n",
      "Successfully built hnswlib\n",
      "Installing collected packages: hnswlib\n",
      "Successfully installed hnswlib-0.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1692  242 5220 3285 5994]]\n",
      "Question:  Batman\n",
      "Top 5 similar movies:\n",
      "====================================================================================================\n",
      "Batman\n",
      "The Dark Knight of Gotham City begins his war on crime with his first major enemy being Jack Napier, a criminal who becomes the clownishly homicidal Joker.\n",
      "====================================================================================================\n",
      "Batman Begins\n",
      "After witnessing his parents' death, Bruce learns the art of fighting to confront injustice. When he returns to Gotham as Batman, he must stop a secret society that intends to destroy the city.\n",
      "====================================================================================================\n",
      "Batman: Gotham by Gaslight\n",
      "In an alternative Victorian Age Gotham City, Batman begins his war on crime while he investigates a new series of murders by Jack the Ripper.\n",
      "====================================================================================================\n",
      "Batman Returns\n",
      "While Batman deals with a deformed man calling himself the Penguin wreaking havoc across Gotham with the help of a cruel businessman, a female employee of the latter becomes the Catwoman with her own vendetta.\n",
      "====================================================================================================\n",
      "Batman: The Movie\n",
      "The Dynamic Duo faces four supervillains who plan to hold the world for ransom with the help of a secret invention that instantly dehydrates people.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import hnswlib\n",
    "class HNSWIndexer:\n",
    "\n",
    "    def __init__(self, sentence_embeddings, ef=50, M=16):\n",
    "        \"\"\"\n",
    "        HNSW Index\n",
    "        \n",
    "        Args:\n",
    "            sentence_embeddings (np.array): Array of sentence embeddings\n",
    "            ef (int): Number of neighbors to inspect during search\n",
    "            M (int): Number of neighbors to keep in graph\n",
    "        \"\"\"\n",
    "        self.ef = ef\n",
    "        self.M = M\n",
    "        self.index = hnswlib.Index(space='cosine', dim=sentence_embeddings.shape[1])\n",
    "        self.index.init_index(max_elements=sentence_embeddings.shape[0], ef_construction=self.ef, M=self.M)\n",
    "        self.index.add_items(sentence_embeddings)\n",
    "\n",
    "    def search(self, query_embedding, top_k=1):\n",
    "        \"\"\"\n",
    "        Search for the most similar sentences to a query embedding\n",
    "\n",
    "        Args:\n",
    "            query_embedding (np.array): Query embedding\n",
    "            top_k (int): Number of similar sentences to return\n",
    "\n",
    "        Returns:\n",
    "            list: List of indices of similar sentences\n",
    "        \"\"\" \n",
    "        self.index.set_ef(self.ef)\n",
    "        labels, distances = self.index.knn_query(query_embedding, k=top_k)\n",
    "        return labels\n",
    "\n",
    "hnsw_indexer = HNSWIndexer(imdb_embeddings)\n",
    "top_k_indices = hnsw_indexer.search(question_embedding, top_k=5)\n",
    "print(top_k_indices)\n",
    "\n",
    "print(\"Question: \", question)\n",
    "print(\"Top 5 similar movies:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for idx in top_k_indices[0]:\n",
    "    idx = idx.item()\n",
    "    print(imdb_dataset[idx][\"Movie Name\"])\n",
    "    print(imdb_dataset[idx][\"Plot\"])\n",
    "    print(\"=\"*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
